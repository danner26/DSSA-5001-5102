---
title: "Linear Regression - DSSA 5201"
owner: "Daniel W. Anner"
output: html_notebook
---

# PseudoCode
# Read in the data
# Divide data into training and test data
# Train the model using the dataset:
#  Create X Matrix [1, xi1, xi2,...,xik] from all rows i of training data and for k variables
#  Create Y Matrix (vector)
# Solve for the β vector, 
# Make predictions using the test dataset:
#  Create X Matrix [1, xi1, xi2,...,xik] from all rows i of test data and for k variables
#  Compute Ŷ = βX
# Test the Model:
#  Compute the Sum of Squared Errors RSS 
#  Compute the Total Sum of Squares TSS 
#  Compute R2 = 1 – RSS / TSS 

```{r}
setwd("E:/_Source/Stockton-DSSA/DSSA-5201_Machine_Learning_Fundamentals/LinearRegression")
# Get the data
df <- read.csv("TrainData_Group1.csv", header = TRUE)
```

```{r}
#Separate data into test and train. Train will be most of data set and test will be the remaining
indexes <- sample(1:nrow(df), size=0.80*nrow(df)) 
#Set data aside for training
train <- df[indexes,]
#Set data aside for testing
test <- df[-indexes,]
#Remove the index variable from memory - helps with larger data sets - no need to retain in memory data we do not need
rm(indexes)
```

```{r}
#Linear regression function
Linear_Regression <- function(df) {
  #Create a column of 1's in the df
  #It should look like the Fit Model: [1 x1 x2 x3 x4 x5 y]
  df <- cbind(rep(1, nrow(df)), df)
  #We will need the length of df to create a subset later
  len <- length(df)
  #X will be the columns including the 1's added but without the Y column, since that will be used to check out answers
  #We need to find what column has the name Y and get its subset with it removed
  X <- subset(df, select=-c(Y))
  #Convert X to a matrix for the Linear Regression Algorithm 
  X <- as.matrix(X)  
  #y is the subset of the column length
  y <- df[,len]   
  #Solve for the β vector
  #%*% is matrix multiplication
  #We will solve a %*% x = b for x, where b can be either a vector or a matrix
  #Important thing here is a numeric, or complex vector, or a matrix giving the right-hand side(s) of the linear system
  #If missing, b is taken to be an identity matrix and solve will return the inverse of a.
  B_hat = solve(t(X) %*% X)  %*% t(X) %*% y #(X^T * X)^-1 * X^T * y 
  return(B_hat)
}
```

```{r}
# Y = βX
# Make predictions using test data
y_hat <- function(df, B_hat) {
   #Create a matrix with the X's, without Y, and matrix multiply it by B_hat
  return (as.matrix(cbind(rep(1, nrow(df)), subset(df, select=-c(Y)))) %*% B_hat) 
}
```

```{r}
#SSE TSS and R^2
calc_errors <- function(y, y_hat, df) {
  #Sum of Squared Errors
  p <- subset(df, select=-c(Y)) #(y - Ŷi)^2 
  p <- ncol(p)
  SSE = sum((y - y_hat)^2)  
  #Total Sum of Squares
  TSS = sum((y - mean(y_hat))^2) #(y - Ŷ_bar)^2
  #R^2
  R2 <- 1 - SSE/TSS #1 - SSE/TSS
  #Adjusted R2 
  adjR2 <- 1 - (SSE/(nrow(df)-p-1)/(TSS/nrow(df)-1))
  adjR2 <- mean(adjR2)
  return(list(R2 = R2, adjR2 = adjR2)) #return the list
}
```

```{r}
#Linear regression test function
Linear_Regression_Testing <- function(train, test){
  #Train the Linear Regression model
  lr_train_model <- lm(data=train, formula = Y~.) 
  #Test the y_hat created by R's B_hat
  lr_prediction = predict(lr_train_model, newdata=test) 
  YPredict <- test$Y
  #Plot the actual data vs R's predicted data 
  plot(x=YPredict, y=lr_prediction, pch = 16, col='blue', main = "Actual VS Predicted by R")
  #Calculate the errors
  errors <- calc_errors(test$Y, lr_prediction, df)
  return(errors$R2)
}
```

```{r}
#Run Linear Regression on the training data 
B_hat <- Linear_Regression(train)

#Make predictions with the test data and the Linear Regression model
#Note: every prediction we make will make the R2 lower 
linear_regression_prediction <- y_hat(test, B_hat)
test$Y
#Plot The actual data vs predicted data 
plot(x=test$Y, y=linear_regression_prediction, pch = 16, col='red', main = "Actual VS Prediction Using Our Linear Regression Model")

#Calculate Error of our model
error <- calc_errors(test$Y, linear_regression_prediction, df)
#Print the data to the user
print(paste("R^2 for our Linear Regression Model: ",error$R2, sep = " "))
print(paste("Adjusted R^2 for our Linear Regression Model: ",error$adjR2, sep = " "))
print(paste("R^2 for R's lm()", Linear_Regression_Testing(train, test), sep = " "))

```

