---
title: "R Notebook"
output: html_notebook
---

## Daniel W. Anner
## Logistic Regression

## PseudoCode
# Read the data
# Divide data into training and test data
# Determine the number of iterations, n
# Train the model using the training dataset
## Create a matrix X from the training data with k features and m observations
## Create the Y matrix (actually a vector) 
## Create a matrix (actually vector), W, of k zeroes (e.g. [0, 0, ., 0] )
## Loop for n iterations
### Compute the sigmoid result, g(x) = 1 / (1 + e^-(WX))
### Compute the gradient = (1/m) * X(g(x) - Y)
### W = W - (0.001 * gradient)
# Make predictions using the test dataset
## Computer Yhat = 1 / 1 + e^-(W^T*X)
# Test the model 

## Import statements and Work Directory
```{r}
library(ggplot2)
library(caTools)
setwd("E:/_Source/Stockton-DSSA/DSSA-5201_Machine_Learning_Fundamentals/LogisticRegression")
```

## Import Data
```{r}
df <- read.csv("LogisticData_1.csv", header = TRUE)
#split data into testing and training
#using caTools like describe in the powerpoint slides 
set.seed(8675309) #https://www.youtube.com/watch?v=6WTdTwcmxyo
#set seed so that the same sample can be reproduced in future
#now selecting 75% of data as sample from total 'n' rows of the data  
indexes = sample.split(df$Y, SplitRatio = 0.75)
#split data into test and train
train = subset(df, indexes == TRUE)
test  = subset(df, indexes == FALSE)
#remove the index variable from memory
rm(indexes)
```

