{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a1bf8a58e309a98fd4c1afd359097b5c1354faf07a92acb9ef2fd44aac5a84d1"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS CODE IS NOT WORKING. THIS IS HOW I WANT TO DO IT, BUT CANT FIGURE IT OUT\n",
    "\n",
    "import autograd.numpy as np\n",
    "import numpy \n",
    "from autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get user impact\n",
    "def get_method():\n",
    "    while True:\n",
    "        method = input(\"Please enter 1 (ReLU) or 2 (Sigmoid) or 3 (Combo) \\n\")\n",
    "        if method in ['1', '2', '3']:\n",
    "            return method\n",
    "method = get_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(object):\n",
    "    def __init__(self, features, number_of_cases):\n",
    "        #Set initial weights and biases\n",
    "        np.random.seed(1)\n",
    "        self.feautres = features\n",
    "        self.weights_1 = np.random.randn(4,5)\n",
    "        self.biases_1 = np.random.randn(4,number_of_cases)\n",
    "        self.weights_2 = np.random.randn(3,4)\n",
    "        self.biases_2 = np.random.randn(3,number_of_cases)\n",
    "        self.weights_3 = np.random.randn(2,3)\n",
    "        self.biases_3 = np.random.randn(2,number_of_cases)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(x, 0, x) #ReLU in place, instead of creating a new var this works on the original matrix\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1.0 + np.exp(-1.0*x)) #sigmoid function\n",
    "\n",
    "    def run_feed_forward(self, method):\n",
    "        if method == 1: #ReLU\n",
    "            # hidden layer 1\n",
    "            hl1 = (np.matmul(self.weights_1, features))\n",
    "            hl1_bias = np.add(hl1, self.biases_1)\n",
    "            hl1_act = self.ReLU(hl1_bias)\n",
    "            # hidden layer 2\n",
    "            hl2 = (np.matmul(self.weights_2, hl1_act))\n",
    "            hl2_bias = np.add(hl2, self.biases_2)\n",
    "            hl2_act = self.ReLU(hl2_bias)\n",
    "            # output layer\n",
    "            output = (np.matmul(self.weights_3, hl2_act))\n",
    "            targets_predicted = np.add(output, self.biases_3)\n",
    "            #Use ReLU for the output activation\n",
    "            targets_predicted = self.ReLU(targets_predicted)\n",
    "        if method == 2:#Sigmoid\n",
    "            # hidden layer 1\n",
    "            hl1 = (np.matmul(self.weights_1, features))\n",
    "            hl1_bias = np.add(hl1, self.biases_1)\n",
    "            hl1_act = self.sigmoid(hl1_bias)\n",
    "            # hidden layer 2\n",
    "            hl2 = (np.matmul(self.weights_2, hl1_act))\n",
    "            hl2_bias = np.add(hl2, self.biases_2)\n",
    "            hl2_act = self.sigmoid(hl2_bias)\n",
    "            # output layer\n",
    "            output = (np.matmul(self.weights_3, hl2_act))\n",
    "            targets_predicted = np.add(output, self.biases_3)\n",
    "            #Use sigmoid for the output activation\n",
    "            targets_predicted = self.sigmoid(targets_predicted)\n",
    "        else:#Combo\n",
    "            # hidden layer 1\n",
    "            hl1 = (np.matmul(self.weights_1, features))\n",
    "            hl1_bias = np.add(hl1, self.biases_1)\n",
    "            hl1_act = self.ReLU(hl1_bias)\n",
    "            # hidden layer 2\n",
    "            hl2 = (np.matmul(self.weights_2, hl1_act))\n",
    "            hl2_bias = np.add(hl2, self.biases_2)\n",
    "            hl2_act = self.ReLU(hl2_bias)\n",
    "            # output layer\n",
    "            output = (np.matmul(self.weights_3, hl2_act))\n",
    "            targets_predicted = np.add(output, self.biases_3)\n",
    "            #Use sigmoid for the output activation\n",
    "            targets_predicted = self.sigmoid(targets_predicted)\n",
    "        return targets_predicted\n",
    "\n",
    "def loss(obj, targets_observed, method):\n",
    "    '''\n",
    "    You will need to add your code here that propagates features\n",
    "    through the network to get predicted_targets\n",
    "    based on matrix multiplication and addition (with weights and biases)\n",
    "    and employing activation functions. I have used w1 to represent the weights\n",
    "    matrix for the transition from the input layer to the first hidden layer\n",
    "    and b1 to represent the biases added at the first hidden layer.\n",
    "    I have used w2 to represent the weights\n",
    "    matrix for the transition from the first hidden layer to the second hidden\n",
    "    layer and b2 to represent the biases added at the second hidden layer.\n",
    "    I have used w3 to represent the weights\n",
    "    matrix for the transition from the second hidden layer to the output\n",
    "    layer and b3 to represent the biases added at the output layer\n",
    "    Usage: Calculate the sum of square residuals of the feed forward function\n",
    "    '''\n",
    "    Targets_predicted = obj.run_feed_forward(method)\n",
    "    print(Targets_predicted)\n",
    "    return np.sum((Targets_predicted - targets_observed) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You selected: 1\n"
     ]
    }
   ],
   "source": [
    "print('You selected: ' + method)\n",
    "\n",
    "# Set up training datam\n",
    "# Each row is a case\n",
    "# Columns 0-4 are features\n",
    "# Columns 5 & 6 are targets\n",
    "\n",
    "features_and_targets = np.array(\n",
    "                                   [[0, 0, 0, 0, 0, 0, 1],\n",
    "                                    [0, 0, 0, 0, 1, 0, 1],\n",
    "                                    [0, 0, 0, 1, 1, 0, 1],\n",
    "                                    [0, 0, 1, 1, 1, 0, 1],\n",
    "                                    [0, 1, 1, 1, 1, 0, 1],\n",
    "                                    [1, 1, 1, 1, 0, 0, 1],\n",
    "                                    [1, 1, 1, 0, 0, 0, 1],\n",
    "                                    [1, 1, 0, 0, 0, 0, 1],\n",
    "                                    [1, 0, 0, 0, 0, 0, 1],\n",
    "                                    [1, 0, 0, 1, 0, 0, 1],\n",
    "                                    [1, 0, 1, 1, 0, 0, 1],\n",
    "                                    [1, 1, 0, 1, 0, 0, 1],\n",
    "                                    [0, 1, 0, 1, 1, 0, 1],\n",
    "                                    [0, 0, 1, 0, 1, 0, 1],\n",
    "                                    [1, 0, 1, 1, 1, 1, 0],\n",
    "                                    [1, 1, 0, 1, 1, 1, 0],\n",
    "                                    [1, 0, 1, 0, 1, 1, 0],\n",
    "                                    [1, 0, 0, 0, 1, 1, 0],\n",
    "                                    [1, 1, 0, 0, 1, 1, 0],\n",
    "                                    [1, 1, 1, 0, 1, 1, 0],\n",
    "                                    [1, 1, 1, 1, 1, 1, 0],\n",
    "                                    [1, 0, 0, 1, 1, 1, 0]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Features: 5\nNumber of Cases: 22\n"
     ]
    }
   ],
   "source": [
    "#shuffle our cases\n",
    "np.random.shuffle(features_and_targets)\n",
    "\n",
    "features = np.transpose(features_and_targets[:,0:5])\n",
    "targets_observed = np.transpose(features_and_targets[:,5:7])\n",
    "\n",
    "number_of_features,number_of_cases = features.shape\n",
    "print(\"Number of Features: %s\" % number_of_features)\n",
    "print(\"Number of Cases: %s\" % number_of_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_Object = FeedForward(features, number_of_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find slope\n",
    "# If you have created a loss function this way then computing the functions\n",
    "# for the gradients of this loss function with respect to all weights\n",
    "# and biases is easy with autograd You can then update the weights\n",
    "# and biases with gradient descent using a learning rate.\n",
    "\n",
    "# grad loss, variable you want to look at\n",
    "\n",
    "d_loss_by_d_w1 = grad(loss, 1)  # w1\n",
    "d_loss_by_d_b1 = grad(loss, 2)  # b1\n",
    "d_loss_by_d_w2 = grad(loss, 3)  # w2\n",
    "d_loss_by_d_b2 = grad(loss, 4)  # b2\n",
    "d_loss_by_d_w3 = grad(loss, 5)  # w3\n",
    "d_loss_by_d_b3 = grad(loss, 6)  # b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.94112395 0.76100376 0.81752363 0.38097733 0.96111357 0.76626409\n  0.85504228 0.81306179 0.82229581 0.70365792 0.64323884 0.92854372\n  0.20883672 0.1898681  0.40750777 0.32694636 0.28221063 0.96230788\n  0.54676025 0.20707037 0.84839242 0.38175414]\n [0.73529568 0.75082573 0.6257793  0.95342505 0.99937968 0.5073614\n  0.81633935 0.82608025 0.99689005 0.52636395 0.91403296 0.99544907\n  0.36684539 0.44051083 0.89273354 0.5698929  0.75716822 0.97106132\n  0.77654353 0.85719168 0.82102531 0.25213498]]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "16.767141411262177"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "loss(NN_Object, targets_observed, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.94112395 0.76100376 0.81752363 0.38097733 0.96111357 0.76626409\n  0.85504228 0.81306179 0.82229581 0.70365792 0.64323884 0.92854372\n  0.20883672 0.1898681  0.40750777 0.32694636 0.28221063 0.96230788\n  0.54676025 0.20707037 0.84839242 0.38175414]\n [0.73529568 0.75082573 0.6257793  0.95342505 0.99937968 0.5073614\n  0.81633935 0.82608025 0.99689005 0.52636395 0.91403296 0.99544907\n  0.36684539 0.44051083 0.89273354 0.5698929  0.75716822 0.97106132\n  0.77654353 0.85719168 0.82102531 0.25213498]]\n(2, 22)\n[[0.94112395 0.76100376 0.81752363 0.38097733 0.96111357 0.76626409\n  0.85504228 0.81306179 0.82229581 0.70365792 0.64323884 0.92854372\n  0.20883672 0.1898681  0.40750777 0.32694636 0.28221063 0.96230788\n  0.54676025 0.20707037 0.84839242 0.38175414]\n [0.73529568 0.75082573 0.6257793  0.95342505 0.99937968 0.5073614\n  0.81633935 0.82608025 0.99689005 0.52636395 0.91403296 0.99544907\n  0.36684539 0.44051083 0.89273354 0.5698929  0.75716822 0.97106132\n  0.77654353 0.85719168 0.82102531 0.25213498]]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,5) (2,22) ",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-4ad989619bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_loss_by_d_w1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_Object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_observed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_Object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md_loss_by_d_w1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_Object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_observed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mNN_Object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_loss_by_d_w1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_Object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_observed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mNN_Object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases_1\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0md_loss_by_d_b1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN_Object\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets_observed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\autograd\\tracer.py\u001b[0m in \u001b[0;36mf_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_box\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf_raw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mf_wrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_autograd_primitive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,5) (2,22) "
     ]
    }
   ],
   "source": [
    "# Set our learning rate\n",
    "lr = 0.00001\n",
    "# Create epoch for our back tracking.\n",
    "# Backpropagate to calculate the gradient for each weight\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    print(d_loss_by_d_w1(NN_Object, targets_observed, method).shape)\n",
    "    print(np.subtract(NN_Object.weights_1,d_loss_by_d_w1(NN_Object, targets_observed, method).dot(lr)))\n",
    "    NN_Object.weights_1 -= lr * d_loss_by_d_w1(NN_Object, targets_observed, method)\n",
    "    NN_Object.biases_1 -= lr * d_loss_by_d_b1(NN_Object, targets_observed, method)\n",
    "    NN_Object.weights_2 -= lr * d_loss_by_d_w2(NN_Object, targets_observed, method)\n",
    "    NN_Object.biases_2 -= lr * d_loss_by_d_b2(NN_Object, targets_observed, method)\n",
    "    NN_Object.weights_3 -= lr * d_loss_by_d_w3(NN_Object, targets_observed, method)\n",
    "    NN_Object.biases_3 -= lr * d_loss_by_d_b3(NN_Object, targets_observed, method)\n",
    "\n",
    "    losses.append(loss(NN_Object, targets_observed, method))\n",
    "\n",
    "    # used for testing purposes. If you want to see how the\n",
    "    # loss backpropagate is calculating a lower gradient run this\n",
    "    print(epoch, loss(NN_Object, targets_observed, method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}